# Fusion Strategies: Combining Multiple Modalities

## Overview

**Fusion Strategies** teaches how to combine embeddings, features, and representations from different modalities into cohesive system outputs. The strategy you choose dramatically affects performance, cost, and latency.

---

## The Fusion Problem

You have multiple modality embeddings. Now what?

```
Image embedding:  (768,)
Text embedding:   (768,)  
Audio embedding:  (768,)

How do you combine them?
```

Three main approaches: **Early Fusion**, **Late Fusion**, and **Hybrid Fusion**.

---

## 1. Early Fusion

**Combine raw inputs before encoding.**

```
Input:
- Image (224Ã—224Ã—3)
- Text (tokens)
- Audio (16kHz waveform)
        â†“
    [Preprocess all]
        â†“
    [Concatenate or interleave]
        â†“
    [Single encoder]
        â†“
Output: (768,) fused embedding
```

### Advantages
âœ… Single unified representation
âœ… Can learn joint patterns early
âœ… Computationally efficient at inference
âœ… Natural alignment (encoder learns temporal sync)

### Disadvantages
âŒ Hard to preprocess (different input types)
âŒ Requires retraining if modality changes
âŒ Encoder must handle all modality types
âŒ Can be unstable (one modality drowns out others)

### Implementation

```python
class EarlyFusionModel(nn.Module):
    def __init__(self):
        # Convert everything to same format
        self.image_preprocessor = ImagePreprocessor()    # â†’ (768,)
        self.text_preprocessor = TextPreprocessor()      # â†’ (768,)
        self.audio_preprocessor = AudioPreprocessor()    # â†’ (768,)
        
        # Single fusion encoder
        self.fusion_encoder = TransformerEncoder(
            input_dim=3*768,  # concatenate all three
            output_dim=768,
            num_layers=6
        )
    
    def forward(self, image, text, audio):
        # Preprocess each modality to same dimension
        img_features = self.image_preprocessor(image)
        txt_features = self.text_preprocessor(text)
        aud_features = self.audio_preprocessor(audio)
        
        # Concatenate
        combined = torch.cat([img_features, txt_features, aud_features], dim=-1)
        
        # Encode together
        fused_embedding = self.fusion_encoder(combined)
        
        return fused_embedding  # (768,)
```

### When to Use
- âœ… You have aligned, synchronized inputs (video + audio)
- âœ… You want lowest latency
- âœ… Modalities are never missing
- âŒ Inputs can be partial or misaligned

---

## 2. Late Fusion

**Encode each modality separately, then combine outputs.**

```
Image â†’ Image Encoder â†’ (768,)
                          â†“
                       [Fusion layer]
Text â†’ Text Encoder â†’ (768,)
                          â†“
                      Combined: (768,)
                          â†‘
Audio â†’ Audio Encoder â†’ (768,)
```

### Advantages
âœ… Modular (each encoder independent)
âœ… Can use pretrained encoders
âœ… Handles missing modalities easily
âœ… More stable (each modality has own signal path)
âœ… Easy to add/remove modalities

### Disadvantages
âŒ Information loss (each encoding is independent)
âŒ Harder to learn cross-modal patterns
âŒ More computation (3 separate forward passes)
âŒ Fusion layer must learn what each modality means

### Implementation

```python
class LateFusionModel(nn.Module):
    def __init__(self):
        # Independent encoders
        self.image_encoder = VisionTransformer(output_dim=768)
        self.text_encoder = BERT(output_dim=768)
        self.audio_encoder = AudioTransformer(output_dim=768)
        
        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(3*768, 1024),
            nn.ReLU(),
            nn.Linear(1024, 768)
        )
    
    def forward(self, image, text, audio):
        # Encode separately
        img_emb = self.image_encoder(image)      # (768,)
        txt_emb = self.text_encoder(text)        # (768,)
        aud_emb = self.audio_encoder(audio)      # (768,)
        
        # Concatenate
        combined = torch.cat([img_emb, txt_emb, aud_emb], dim=-1)
        
        # Fuse
        fused = self.fusion(combined)  # (768,)
        
        return fused
    
    def forward_partial(self, **kwargs):
        """Handle missing modalities"""
        embeddings = []
        
        if 'image' in kwargs:
            embeddings.append(self.image_encoder(kwargs['image']))
        if 'text' in kwargs:
            embeddings.append(self.text_encoder(kwargs['text']))
        if 'audio' in kwargs:
            embeddings.append(self.audio_encoder(kwargs['audio']))
        
        combined = torch.cat(embeddings, dim=-1)
        
        # Fusion layer must handle variable sizes
        # Use adaptive pooling or padding
        fused = self.adaptive_fusion(combined)
        return fused
```

### When to Use
- âœ… You want modularity
- âœ… Some modalities may be missing
- âœ… You want to leverage pretrained models
- âœ… Modalities are independent
- âŒ Need tight cross-modal coupling

---

## 3. Hybrid Fusion

**Combine early and late fusion.**

```
Image â†’ Early Fusion â†’ (384,)
      â†—    (with audio)     â†˜
Audio â†’ Early Fusion â”€â”€â”€â”€â†’ Late Fusion â†’ (768,)
                              â†‘
                               â†“
Text â†’ Text Encoder â†’ (384,) â”€â”€â”€
```

### Advantages
âœ… Best of both worlds
âœ… Early fusion captures tight sync (audio-visual)
âœ… Late fusion handles modularity (text)
âœ… Can use pretrained encoders
âœ… More flexible

### Disadvantages
âŒ More complex
âŒ More parameters
âŒ Harder to debug

### Implementation

```python
class HybridFusionModel(nn.Module):
    def __init__(self):
        # Early fusion: tightly coupled modalities
        self.image_encoder = VisionTransformer(output_dim=512)
        self.audio_encoder = AudioTransformer(output_dim=512)
        self.audiovisual_fusion = AudioVisualFusionModule(output_dim=384)
        
        # Late fusion: independent modality
        self.text_encoder = BERT(output_dim=384)
        
        # Final fusion
        self.final_fusion = nn.Sequential(
            nn.Linear(384 + 384, 1024),
            nn.ReLU(),
            nn.Linear(1024, 768)
        )
    
    def forward(self, image, text, audio):
        # Early fusion: audio-visual
        img_emb = self.image_encoder(image)
        aud_emb = self.audio_encoder(audio)
        av_fused = self.audiovisual_fusion(img_emb, aud_emb)  # (384,)
        
        # Late: text
        txt_emb = self.text_encoder(text)  # (384,)
        
        # Final fusion
        combined = torch.cat([av_fused, txt_emb], dim=-1)
        output = self.final_fusion(combined)  # (768,)
        
        return output
```

### When to Use
- âœ… Some modalities naturally go together (audio-visual)
- âœ… Others are independent (text)
- âœ… Want efficiency and modularity
- âœ… Complex applications

---

## 4. Cross-Attention Fusion

**Explicitly model modality interactions using attention.**

```
Image Features â†’ Attention Head
                    â†“
                [Cross-Attention]
                    â†‘
Text Features â†’ Attention Head
```

### How It Works

```python
class CrossAttentionFusion(nn.Module):
    def __init__(self, dim=768):
        self.image_to_text = nn.MultiheadAttention(dim, num_heads=8)
        self.text_to_image = nn.MultiheadAttention(dim, num_heads=8)
        self.self_attention = nn.MultiheadAttention(dim, num_heads=8)
    
    def forward(self, image_emb, text_emb, audio_emb):
        # Image attends to text
        enhanced_image = self.image_to_text(
            query=image_emb,
            key=text_emb,
            value=text_emb
        )
        
        # Text attends to image
        enhanced_text = self.text_to_image(
            query=text_emb,
            key=image_emb,
            value=image_emb
        )
        
        # All together self-attend
        combined = torch.stack([enhanced_image, enhanced_text, audio_emb])
        fused = self.self_attention(
            query=combined,
            key=combined,
            value=combined
        )
        
        # Aggregate
        output = fused.mean(dim=0)  # (768,)
        return output
```

### Advantages
âœ… Explicitly models modality relationships
âœ… Learns what to attend to
âœ… Very flexible
âœ… State-of-the-art performance

### Disadvantages
âŒ Higher computation cost
âŒ Requires more training data
âŒ More parameters = more overfitting risk

### When to Use
- âœ… You have good training data
- âœ… Interaction patterns matter
- âœ… Latency is less critical
- âœ… You want best possible performance

---

## Comparison Table

| Strategy | Latency | Modularity | Performance | Complexity | Handles Missing |
|----------|---------|-----------|-------------|-----------|-----------------|
| Early | âš¡âš¡âš¡ | â­ | â­â­ | ğŸŸ¢ Simple | âŒ No |
| Late | âš¡âš¡ | â­â­â­ | â­â­ | ğŸŸ¡ Medium | âœ… Yes |
| Hybrid | âš¡âš¡ | â­â­ | â­â­â­ | ğŸ”´ Complex | âœ… Partial |
| Cross-Attention | âš¡ | â­â­â­ | â­â­â­â­ | ğŸ”´ Very Complex | âœ… Yes |

---

## Choosing Your Fusion Strategy

```
Do modalities arrive simultaneously/synchronized?
â”œâ”€ YES â†’ Consider Early Fusion for efficiency
â””â”€ NO â†’ Use Late Fusion for flexibility

Do you need to handle missing modalities?
â”œâ”€ YES â†’ Late or Cross-Attention
â””â”€ NO â†’ Any strategy works

Do you need maximum performance?
â”œâ”€ YES â†’ Cross-Attention Fusion
â””â”€ NO â†’ Late Fusion (good balance)

Do you have latency constraints?
â”œâ”€ STRICT â†’ Early Fusion
â”œâ”€ MODERATE â†’ Late Fusion
â””â”€ LOOSE â†’ Cross-Attention
```

---

## Practical Example: Implementing All Three

See [scripts/fusion_comparison.py](scripts/fusion_comparison.py) for complete working examples comparing all strategies on a benchmark task.

---

## Common Pitfalls

âŒ **Using Late Fusion when inputs are temporally synchronized**
- You'll lose temporal alignment
- âœ… Use Early or Hybrid instead

---

âŒ **Not normalizing embeddings before concatenation**
```python
# DON'T:
combined = torch.cat([image_emb, text_emb, audio_emb])

# DO:
combined = torch.cat([
    F.normalize(image_emb, p=2, dim=-1),
    F.normalize(text_emb, p=2, dim=-1),
    F.normalize(audio_emb, p=2, dim=-1)
])
```

---

âŒ **Treating all modalities equally**
- One modality might be noisier
- âœ… Learn modality-specific weights:
```python
weights = nn.Parameter(torch.ones(3))
weighted_combined = torch.cat([
    weights[0] * image_emb,
    weights[1] * text_emb,
    weights[2] * audio_emb
])
```

---

## Advanced: Gated Fusion

Give the model control over how much each modality contributes:

```python
class GatedFusion(nn.Module):
    def forward(self, image_emb, text_emb, audio_emb):
        combined = torch.cat([image_emb, text_emb, audio_emb], dim=-1)
        
        # Learn gates
        gate_image = torch.sigmoid(self.gate_image_net(combined))
        gate_text = torch.sigmoid(self.gate_text_net(combined))
        gate_audio = torch.sigmoid(self.gate_audio_net(combined))
        
        # Gated combination
        output = (
            gate_image * image_emb +
            gate_text * text_emb +
            gate_audio * audio_emb
        )
        
        return output
```

---

## Key Takeaways

1. **No universally best strategy** - choose based on your constraints
2. **Early Fusion** = efficiency but less flexible
3. **Late Fusion** = modularity, handles missing data
4. **Hybrid** = balance
5. **Cross-Attention** = best performance if you have compute
6. **Always normalize** before concatenation
7. **Modalities aren't equal** - consider weighting

---

## Next Steps

1. See [examples/vision-language-chat](../../examples/vision-language-chat) for Late Fusion in action
2. Study [evaluation](../evaluation) for how to benchmark fusion strategies
3. Read [cost-optimization](../cost-optimization) for efficient fusion at scale

---

## References

- Multimodal Machine Learning: A Survey and Taxonomy (BaltruÅ¡aitis et al., 2018)
- Transformers Can Do Bayesian Inference (Spantidakis et al., 2024)
- Vision-Language Pre-training (Alayrac et al., 2022)
